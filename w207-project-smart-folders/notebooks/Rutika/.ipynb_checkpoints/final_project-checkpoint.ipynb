{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named nltk.tokenize",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-eebc131245a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRegexpTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_stop_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mporter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named nltk.tokenize"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emails = pd.read_csv(\"../../source-data/emails.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data():\n",
    "    df = pd.DataFrame(columns=['date',\n",
    "                               'sender_address',\n",
    "                               'recipient',\n",
    "                               'subject',\n",
    "                               'sender_name',\n",
    "                               'recipient_name',\n",
    "                               'cc',\n",
    "                               'bcc',\n",
    "                               'folder',\n",
    "                               'body'])\n",
    "    \n",
    "    for index, details in emails.iterrows():\n",
    "        raw_email_info = details['message']\n",
    "        # raw_email_info = f_in.read().decode('utf8')\n",
    "        date = re.findall(r'Date: (.*)', raw_email_info)[0]\n",
    "        sender_address = re.findall(r'From: (.*)', raw_email_info)[0]\n",
    "        recipient = re.findall(r'To: (.*)', raw_email_info)[0]\n",
    "        subject = re.findall(r'Subject: (.*)', raw_email_info)[0]\n",
    "        sender_name = re.findall(r'X-From: (.*)', raw_email_info)[0]\n",
    "        recipient_name = re.findall(r'X-To: ([ A-Za-z]*)', raw_email_info)[0]\n",
    "        cc = re.findall(r'X-cc: (.*)', raw_email_info)[0]\n",
    "        bcc = re.findall(r'X-bcc: (.*)', raw_email_info)[0]\n",
    "        folder = re.findall(r'\"[a-zA-z-]*/(.*)/.*,\"Message-ID.*>', raw_email_info)\n",
    "        \n",
    "        # strip everything before X-FileName\n",
    "        bodies = re.findall(r'(?<=X-FileName: )(?s)(.*$)', raw_email_info)\n",
    "        # Get everything after the first newline\n",
    "        bodies = bodies[0].split('\\n')\n",
    "        body = \" \".join(bodies[1:])\n",
    "        \n",
    "        df.loc[index] = [date, sender_address, recipient, subject, sender_name, recipient_name, cc, bcc, folder, body]\n",
    "        \n",
    "    return df\n",
    "                         \n",
    "m = extract_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14, 10)\n",
      "0                               Here is our forecast   \n",
      "1      Traveling to have a business meeting takes th...\n",
      "2                        test successful.  way to go!!!\n",
      "3      Randy,   Can you send me a schedule of the sa...\n",
      "4                   Let's shoot for Tuesday at 11:45.  \n",
      "5      Greg,   How about either next Tuesday or Thur...\n",
      "6      Please cc the following distribution list wit...\n",
      "7                      any morning between 10 and 11:30\n",
      "8      1. login:  pallen pw: ke9davis   I don't thin...\n",
      "9      ---------------------- Forwarded by Phillip K...\n",
      "10     Mr. Buckner,   For delivered gas behind San D...\n",
      "11     Lucy,   Here are the rentrolls:     Open them...\n",
      "12     ---------------------- Forwarded by Phillip K...\n",
      "13     ---------------------- Forwarded by Phillip K...\n",
      "Name: body, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(m.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['travel',\n",
       " 'busi',\n",
       " 'meet',\n",
       " 'take',\n",
       " 'fun',\n",
       " 'trip',\n",
       " 'especi',\n",
       " 'prepar',\n",
       " 'present',\n",
       " 'suggest',\n",
       " 'hold',\n",
       " 'busi',\n",
       " 'plan',\n",
       " 'meet',\n",
       " 'take',\n",
       " 'trip',\n",
       " 'without',\n",
       " 'formal',\n",
       " 'busi',\n",
       " 'meet',\n",
       " 'even',\n",
       " 'tri',\n",
       " 'get',\n",
       " 'honest',\n",
       " 'opinion',\n",
       " 'whether',\n",
       " 'trip',\n",
       " 'even',\n",
       " 'desir',\n",
       " 'necessari',\n",
       " 'far',\n",
       " 'busi',\n",
       " 'meet',\n",
       " 'think',\n",
       " 'product',\n",
       " 'tri',\n",
       " 'stimul',\n",
       " 'discuss',\n",
       " 'across',\n",
       " 'differ',\n",
       " 'group',\n",
       " 'work',\n",
       " 'often',\n",
       " 'present',\n",
       " 'speak',\n",
       " 'other',\n",
       " 'quiet',\n",
       " 'just',\n",
       " 'wait',\n",
       " 'turn',\n",
       " 'meet',\n",
       " 'might',\n",
       " 'better',\n",
       " 'held',\n",
       " 'round',\n",
       " 'tabl',\n",
       " 'discuss',\n",
       " 'format',\n",
       " 'suggest',\n",
       " 'go',\n",
       " 'austin',\n",
       " 'play',\n",
       " 'golf',\n",
       " 'rent',\n",
       " 'ski',\n",
       " 'boat',\n",
       " 'jet',\n",
       " 'ski',\n",
       " 'fli',\n",
       " 'somewher',\n",
       " 'take',\n",
       " 'much',\n",
       " 'time']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in m['body']:\n",
    "    #print \"Processing\",i\n",
    "    # clean and tokenize document string\n",
    "    tokens = tokenizer.tokenize(i)\n",
    "    # remove all numbers\n",
    "    tokens = [x for x in tokens if not (x.isdigit() or x[0] == '-' and x[1:].isdigit())]\n",
    "    # remove structural words\n",
    "    tokens = [x for x in tokens if len(x) > 1]\n",
    "    tokens = [x.lower() for x in tokens]\n",
    "    tokens = [x for x in tokens if 'http' not in x]\n",
    "    tokens = [x for x in tokens if x not in \"_\"]\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    # add tokens to list\n",
    "    texts.append(stemmed_tokens)\n",
    "\n",
    "dictionaryall = corpora.Dictionary(texts)\n",
    "\n",
    "corpusall = [dictionaryall.doc2bow(text) for text in texts]\n",
    "\n",
    "texts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['let', 'shoot', 'tuesday']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rutika/anaconda3/lib/python3.6/site-packages/gensim/models/ldamodel.py:497: RuntimeWarning: overflow encountered in exp\n",
      "  expElogthetad = np.exp(Elogthetad)\n"
     ]
    }
   ],
   "source": [
    "ldamodelall = gensim.models.ldamodel.LdaModel(corpusall, num_topics=7, id2word = dictionaryall, passes=20,\n",
    "                                              minimum_probability=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4, 'nan*\"direct\" + nan*\"determin\" + nan*\"document\" + nan*\"difficult\" + nan*\"down\"'), (3, 'nan*\"direct\" + nan*\"determin\" + nan*\"document\" + nan*\"difficult\" + nan*\"down\"'), (2, 'nan*\"direct\" + nan*\"determin\" + nan*\"document\" + nan*\"difficult\" + nan*\"down\"'), (0, 'nan*\"direct\" + nan*\"determin\" + nan*\"document\" + nan*\"difficult\" + nan*\"down\"'), (6, 'nan*\"direct\" + nan*\"determin\" + nan*\"document\" + nan*\"difficult\" + nan*\"down\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodelall.print_topics(num_topics=5, num_words=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
